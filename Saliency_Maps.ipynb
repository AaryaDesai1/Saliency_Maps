{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/NEnR3cIyfGvch0a6X7HU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaryaDesai1/Saliency_Maps/blob/main/Saliency_Maps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Introduction](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=c6Mk8HpKa9Aw)\n",
        "\n",
        ">>[What are Saliency Maps?](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=c6Mk8HpKa9Aw)\n",
        "\n",
        ">>[How Saliecy maps work:](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=c6Mk8HpKa9Aw)\n",
        "\n",
        ">>[How to interpret the saliency map:](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=c6Mk8HpKa9Aw)\n",
        "\n",
        ">[Hypotheses](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=-QmQKJJu3J3H)\n",
        "\n",
        ">>[Null Hypothesis (H₀):](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=-QmQKJJu3J3H)\n",
        "\n",
        ">>[Alternative Hypothesis (H₁):](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=-QmQKJJu3J3H)\n",
        "\n",
        ">[Setting up the Notebook](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=psJHNQ2LfMDo)\n",
        "\n",
        ">>[Installing and Importing libraries](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=psJHNQ2LfMDo)\n",
        "\n",
        ">>[GPU Set-Up and Modelling](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=B-N0PZvJfxjX)\n",
        "\n",
        ">>>[Checking for GPU Availability](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=B-N0PZvJfxjX)\n",
        "\n",
        ">>>[Loading the CIFAR-10 Dataset](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=B-N0PZvJfxjX)\n",
        "\n",
        ">>>[Loading a Pre-trained ResNet-18 Model](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=B-N0PZvJfxjX)\n",
        "\n",
        ">[Saliency Mapping](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=EGzQqbJ3gbxi)\n",
        "\n",
        ">>[Defining Boundary & Internal Mask](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=zJwp-S67ggKM)\n",
        "\n",
        ">[Statistical Hypothesis Testing](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=jZFxZ0sTgmgY)\n",
        "\n",
        ">[Results](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=D6kqSDiFgrhF)\n",
        "\n",
        ">[Interpretation of Results:](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=TUfy7IZjolfL)\n",
        "\n",
        ">>[Statistical Results](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=TUfy7IZjolfL)\n",
        "\n",
        ">>[Image results:](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=TUfy7IZjolfL)\n",
        "\n",
        ">>[Conclusion:](#updateTitle=true&folderId=1x36IERwUJKrneEXL5cf-qzCS5IjSGk3Q&scrollTo=TUfy7IZjolfL)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "mgeu8kIEl6mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note** ChatGPT and Co-Pilot were used to write the code and interpret the results.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This assignment focuses on hypothesis testing, specifically in deep learning. For the same, I chose to look into how convolutional neural networks (CNNs) process and identify features in image classification tasks using saliency maps. The hypotheses are put forth as follows:\n",
        "\n",
        "## What are Saliency Maps?\n",
        "A saliency map is a visualization technique that highlights the most important pixels in an image that contribute to a model's prediction. It typically shows which areas of the image have the greatest influence on the output, often by computing the gradient of the output with respect to the input pixels.\n",
        "\n",
        "## How Saliecy maps work:\n",
        "1. Saliency maps are used to visualize the importance of each input feature to the model's prediction.\n",
        "2. The saliency map is generated by computing the gradient of the model's output with respect to the input features.\n",
        "3. The gradient values indicate how much the model's output would change if the input features were changed slightly.\n",
        "4. The saliency map highlights the input features that have the most impact on the model's prediction.\n",
        "5. Saliency maps are useful for interpreting and understanding the model's decision-making process.\n",
        "\n",
        "## How to interpret the saliency map:\n",
        "1. Bright regions in the saliency map indicate input features that are important for the model's prediction.\n",
        "2. Dark regions in the saliency map indicate input features that are less important for the model's prediction.\n",
        "3. The saliency map can help identify which input features the model is focusing on when making a prediction.\n",
        "4. By analyzing the saliency map, we can gain insights into the model's decision-making process and understand how it is using the input features to make predictions.\n",
        "5. Saliency maps can help identify potential biases or errors in the model's predictions by highlighting the input features that are driving the model's decisions.\n"
      ],
      "metadata": {
        "id": "c6Mk8HpKa9Aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypotheses\n",
        "\n",
        "## Null Hypothesis (H₀):\n",
        "There is no significant (p > .05) difference in the importance of pixels around an object’s boundaries compared to pixels within the object for classifying images of animals in the ResNet model.\n",
        "\n",
        "## Alternative Hypothesis (H₁):\n",
        "There is a significant difference (p < .05) in the importance of pixels around an object’s boundaries compared to pixels within the object for classifying images of animals in the ResNet model. Specifically, pixels around the boundaries are more important."
      ],
      "metadata": {
        "id": "-QmQKJJu3J3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Notebook\n",
        "\n",
        "## Installing and Importing libraries\n",
        "- `torch`: PyTorch, used for building and running deep learning models.\n",
        "- `torchvision`: Contains utilities for vision-based tasks, including pre-trained models and datasets like CIFAR-10.\n",
        "- `matplotlib`: Used for plotting and visualizing results, such as saliency maps.\n",
        "- `numpy`: Fundamental package for numerical computations.\n",
        "- `cv2` (OpenCV): Used for image processing, including generating boundary masks.\n",
        "- `scipy`: Provides statistical functions, used here for hypothesis testing."
      ],
      "metadata": {
        "id": "psJHNQ2LfMDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.25.2 matplotlib==3.7.1 tensorflow==2.14.1\n",
        "!pip install torch torchvision scipy opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GdkvNyckfYX8",
        "outputId": "b7ad3607-47a0-4ba9-c7b1-d9eed69ce9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting tensorflow==2.14.1\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (18.1.1)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow==2.14.1)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.14.1)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.1) (1.64.1)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow==2.14.1)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow==2.14.1)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow==2.14.1)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.1) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.1) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow==2.14.1)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.1) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.1) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.1) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.1) (3.2.2)\n",
            "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, numpy, keras, ml-dtypes, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorstore 0.1.66 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.0.0 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.25.2 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "35cac1e3c5684da6bf9b3c36a713675b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN5WuxtW27NK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU Set-Up and Modelling\n",
        "\n",
        "In this section, we set up the GPU for faster computations (if available) and load the CIFAR-10 dataset, followed by loading a pre-trained ResNet-18 model.\n",
        "\n",
        "### Checking for GPU Availability\n",
        "We use PyTorch's `torch.device` to check if a GPU is available. If CUDA is supported on the machine, the model will run on the GPU; otherwise, it defaults to the CPU. Utilizing a GPU significantly speeds up the process of deep learning tasks, especially when working with larger models and datasets.\n",
        "\n",
        "### Loading the CIFAR-10 Dataset\n",
        "We load the CIFAR-10 dataset, a standard image classification dataset containing 10 different classes (e.g., cats, dogs, trucks). Here, the dataset is loaded in test mode with images being converted into tensors using `torchvision.transforms`.\n",
        "\n",
        "### Loading a Pre-trained ResNet-18 Model\n",
        "We load a pre-trained ResNet-18 model from torchvision. This model has been trained on the large ImageNet dataset and is used here in evaluation mode to extract saliency maps and perform experiments without additional training. The model is moved to the GPU (if available) for faster computations.\n",
        "\n",
        "These steps make the model ready to be used for saliency maps."
      ],
      "metadata": {
        "id": "B-N0PZvJfxjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slz1dNkPfwMz",
        "outputId": "bf885850-f751-4acf-f73f-767311762d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10 class labels\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "xqGvb3bYltb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saliency Mapping\n",
        "\n",
        "The `generate_saliency_map` function computes a saliency map for a given input image and target class. Saliency maps highlight which pixels in the image most strongly influence the model’s prediction.\n",
        "\n",
        "1. **Input Preparation**: The input image is moved to the device (GPU/CPU) and set to require gradient calculations, allowing us to compute gradients with respect to the input pixels.\n",
        "2. **Forward Pass**: The model processes the image, and the score corresponding to the target class is extracted from the model's output.\n",
        "3. **Backward Pass**: Gradients of the target class score are computed with respect to the input image by backpropagating the model's prediction. This shows how changes in each pixel affect the model’s decision.\n",
        "4. **Saliency Calculation**: The absolute gradients are taken to capture pixel importance, and the maximum gradient across channels is used for multi-channel images like RGB. The saliency map is then converted to a NumPy array for visualization.\n",
        "\n",
        "The result is a saliency map that highlights the most critical pixels influencing the model’s classification decision."
      ],
      "metadata": {
        "id": "EGzQqbJ3gbxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate saliency map\n",
        "def generate_saliency_map(model, input_image, target_class):\n",
        "    input_image = input_image.to(device)\n",
        "    input_image.requires_grad_()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_image)\n",
        "\n",
        "    # Get the score for the target class\n",
        "    score = output[0, target_class]\n",
        "\n",
        "    # Backward pass to get the gradients\n",
        "    model.zero_grad()\n",
        "    score.backward()\n",
        "\n",
        "    # Get the gradient and compute absolute value\n",
        "    saliency, _ = torch.max(input_image.grad.data.abs(), dim=1)\n",
        "    saliency = saliency.squeeze().cpu().numpy()\n",
        "\n",
        "    return saliency\n"
      ],
      "metadata": {
        "id": "t6C-BHFAgeOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Boundary & Internal Mask\n",
        "\n",
        "The `get_boundary_mask` function generates two masks for an input image: one for boundary pixels and one for internal pixels. This helps in analyzing which parts of the image are more important for the model's decision.\n",
        "\n",
        "1. **Grayscale Conversion**: The input image is converted to grayscale to simplify edge detection.\n",
        "2. **Edge Detection**: The Canny edge detection algorithm is applied to the grayscale image, identifying boundary pixels where edges exist.\n",
        "3. **Boundary and Internal Masks**: A boundary mask is created where edges are detected, while the inverse of this mask is used to identify internal pixels (non-edges).\n",
        "\n",
        "These masks allow for the separation of boundary and internal regions of the image for further analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zJwp-S67ggKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get boundary and internal masks\n",
        "def get_boundary_mask(image):\n",
        "    image_uint8 = (image * 255).astype(np.uint8)  # Scale and convert to uint8\n",
        "    grayscale_image = cv2.cvtColor(image_uint8, cv2.COLOR_RGB2GRAY)\n",
        "    edges = cv2.Canny(grayscale_image, 100, 200)\n",
        "    boundary_mask = (edges > 0).astype(float)\n",
        "    internal_mask = (edges == 0).astype(float)\n",
        "    return boundary_mask, internal_mask\n"
      ],
      "metadata": {
        "id": "181h3uZtgiq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Hypothesis Testing\n",
        "\n",
        "This code snippet analyzes the importance of boundary versus internal pixels using saliency maps and performs a statistical comparison.\n",
        "\n",
        "1. **Initialization**: Two lists, `boundary_importances` and `internal_importances`, are created to store average saliency scores for boundary and internal pixels, respectively.\n",
        "\n",
        "2. **Loop Through Test Images**: The code processes the first 100 images from the test loader:\n",
        "   - For each image, the saliency map is generated using the pre-trained model and the corresponding label.\n",
        "   - Boundary and internal masks are obtained for the image.\n",
        "\n",
        "3. **Saliency Calculation**: The average saliency values are computed for both boundary and internal regions by multiplying the saliency map with the respective masks and calculating the mean.\n",
        "\n",
        "4. **Statistical Comparison**: A paired t-test is performed to compare the importance of boundary pixels against internal pixels. The t-statistic and p-value are printed, providing insights into whether there is a statistically significant difference in their importance.\n",
        "\n",
        "This analysis helps determine if boundary or internal pixels play a more crucial role in the model's predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jZFxZ0sTgmgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boundary_importances = []\n",
        "internal_importances = []\n",
        "\n",
        "for i, (images, labels) in enumerate(testloader):\n",
        "    if i == 100:  # Limiting to first 100 images for simplicity\n",
        "        break\n",
        "\n",
        "    # Get the image and label\n",
        "    image = images[0].permute(1, 2, 0).numpy()  # Convert to HxWxC\n",
        "    label = labels[0].item()\n",
        "\n",
        "    # Generate the saliency map\n",
        "    saliency = generate_saliency_map(model, images, label)\n",
        "\n",
        "    # Get boundary and internal masks\n",
        "    boundary_mask, internal_mask = get_boundary_mask(image * 255)\n",
        "\n",
        "    # Calculate the average saliency for boundary and internal regions\n",
        "    boundary_saliency = np.mean(saliency * boundary_mask)\n",
        "    internal_saliency = np.mean(saliency * internal_mask)\n",
        "\n",
        "    boundary_importances.append(boundary_saliency)\n",
        "    internal_importances.append(internal_saliency)\n",
        "\n",
        "# Perform a t-test to compare the importance of boundary vs internal pixels\n",
        "t_stat, p_value = stats.ttest_rel(boundary_importances, internal_importances)\n",
        "print(f\"T-statistic: {t_stat:.3f}, P-value: {p_value:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh8JTJJ8gpui",
        "outputId": "1cdbdca2-df90-4559-9f89-5ef59195d757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-statistic: -27.649, P-value: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "The `visualize` function displays the original image, its corresponding saliency map, and the boundary and internal masks in a single figure. This visualization helps in understanding how different regions of the image contribute to the model's predictions.\n",
        "\n",
        "1. **Subplot Creation**: A figure with four subplots is created to accommodate the original image, saliency map, boundary mask, and internal mask.\n",
        "\n",
        "2. **Displaying Images**:\n",
        "   - The original image is shown in the first subplot.\n",
        "   - The saliency map is displayed in the second subplot using a 'hot' colormap to emphasize important areas.\n",
        "   - The boundary and internal masks are shown in the third and fourth subplots, respectively, using a grayscale colormap.\n",
        "\n",
        "3. **Formatting**: The axes are turned off for all subplots to provide a cleaner view, and the final visualization is displayed using `plt.show()`.\n",
        "\n",
        "An example is provided where the function visualizes the first image in the test loader along with its saliency and masks, illustrating the model's focus areas in the image."
      ],
      "metadata": {
        "id": "D6kqSDiFgrhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(image, saliency, boundary_mask, internal_mask):\n",
        "    fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
        "\n",
        "    ax[0].imshow(image)\n",
        "    ax[0].set_title('Original Image')\n",
        "\n",
        "    ax[1].imshow(saliency, cmap='hot')\n",
        "    ax[1].set_title('Saliency Map')\n",
        "\n",
        "    ax[2].imshow(boundary_mask, cmap='gray')\n",
        "    ax[2].set_title('Boundary Mask')\n",
        "\n",
        "    ax[3].imshow(internal_mask, cmap='gray')\n",
        "    ax[3].set_title('Internal Mask')\n",
        "\n",
        "    for a in ax:\n",
        "        a.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Fetch and visualize the first image\n",
        "images, labels = next(iter(testloader))\n",
        "image = images[0].permute(1, 2, 0).numpy()  # Convert to HxWxC format\n",
        "saliency = generate_saliency_map(model, images, labels[0].item())\n",
        "boundary_mask, internal_mask = get_boundary_mask(image)\n",
        "\n",
        "# Visualize saliency, boundary, and internal masks\n",
        "visualize(image, saliency, boundary_mask, internal_mask)\n",
        "\n",
        "# Print the class label of the image\n",
        "print(f\"Label: {labels[0].item()}, Class: {classes[labels[0].item()]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "AD6HPzeegsfJ",
        "outputId": "30ed820b-a75d-4ed4-90ea-0fb7bc7951a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 3, Class: cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretation of Results:\n",
        "## Statistical Results\n",
        "The t-test comparing the saliency between boundary and internal pixels yielded a T-statistic of -27.649 and a P-value of 0.000.\n",
        "\n",
        "- T-statistic (-27.649): The large negative value indicates that the average saliency for internal pixels is significantly greater than the saliency for boundary pixels. This strong difference is likely to be meaningful given the magnitude of the statistic.\n",
        "- P-value (0.000): The P-value is extremely small, effectively zero. This means the observed difference is statistically significant, and we can reject the null hypothesis with a high degree of confidence.\n",
        "\n",
        "## Image results:\n",
        "The images of the cat in the last plot reveal that the saliency map highlights the internal features, such as the face or body of the cat, as being more important to the model’s prediction than the edges or boundaries. The boundary mask shows the cat’s outline, while the internal mask covers areas within the outline. Given the results (T-statistic: -27.649, P-value: 0.000), the plot visually supports the conclusion that the model places significantly more emphasis on the internal parts of the cat, such as its face, rather than the edges or contours.\n",
        "\n",
        "## Conclusion:\n",
        "The experiment suggests that internal pixels of the images tend to have higher importance (saliency) compared to boundary pixels, as per the deep learning model's saliency map. This finding aligns with the hypothesis that the model focuses more on the internal features of objects rather than their edges or boundaries when making predictions."
      ],
      "metadata": {
        "id": "TUfy7IZjolfL"
      }
    }
  ]
}